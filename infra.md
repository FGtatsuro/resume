# テストインフラの構築

- チームで利用するインフラの Infrastructure as Code(IaC)を主導した。
- クラウド(GCP)環境への移行を主導した。

## 背景

- 活動に取り組む前までは、チームで利用するインフラ(Jenkinsなど)は全て手動で構築していた。
   - Jenkinsはmaster/slave構成を取る。masterは1台であり構築も一回で済むが、slaveは増やす度に構築作業が入るため、作業コストが高く気軽に増やせなかった。
- 構築を任せられた個人が作業を元にスクリプトを書く事はあったが、それがチーム全体として共有されていなかった。
   - 以前の構築作業の知見が引き継がれなかった。

---

## CI環境のコード化(2015/04〜)

CIとして使用していたJenkinsのmaster/slaveのコンテナイメージを作成・起動するコードを作成した。
個人プロジェクトとしてスタートした後、チームのプロジェクトに組み込まれて現在も使われている。

### 技術スタックと選定理由

- Ansible: コンテナイメージのプロビジョニングを行う
   - 以下の2点により、Dockerfileによるプロビジョニングをメインの手段とはしなかった。
      - Dockerfileに記述するには処理が多すぎた。
      - チームリーダにPoCの段階でデモをした際、コンテナ環境だけでなく実マシンへのプロビジョニングもできるようにして欲しいとの依頼を受けた。
   - Ansible以外の選択肢として、Puppet、Chefがあった。自分自身はどちらも使用経験があったが、他のチームメンバーがどれも使用経験がない状態で、以下の判断によりAnsibleを選んだ。
      - Puppet: DSLの癖がやや強く、未経験の状態から慣れるまでに時間がかかるだろうと判断した。また、マスター/エージェント構成を取る関係上、それらの構築も必要となり敷居が高かった。
      - Chef: RubyのDSLが使え非常に柔軟な設定が可能になる反面、Rubyに慣れていないメンバー(自分自身含む)にはその柔軟さを持て余す危険があった。
      - Ansible: Python製であるが、メインの設定はYAMLであり、特定のプログラミング言語の知識は必要なかった。そのため、チームメンバーにも扱いやすいだろうと判断した。
- Packer: ProvisionerとしてAnsibleを指定し、master/slaveのコンテナイメージを作成する。
- Docker compose: 作成したmaster/slaveのコンテナイメージを起動する。
   - コンテナオーケストレーションのツールとしては、当時はこれぐらいしか選択肢が(知りうる限りでは)なかった。

### もたらした改善/成果

- CI環境の動作前提がほぼDockerだけになり、環境構築が非常に簡単になった。
   - 機密情報は手動で設定する部分が残っている。
   - 導入後にハードディスクが壊れた事があったが、容易に復旧できた。
- Docker composeの設定ファイルに数行追加し再起動するだけでslaveの追加が行えるようになり、気軽に増やせるようになった。
   - slaveの追加 => 同時に実行するテストジョブ数の増加 => テスト総時間の短縮
- コード化したことにより、環境に変更を加える前にPRレビューが実施できるようになった。

### 残った/生じた課題

- コンテナの思想(ex. なるべく小さなサイズで、1コンテナ-1サービス、など)を理解しておらず、仮想マシンの延長で考えてしまった。
   - テストに必要なランタイムを全て1つのイメージに詰め込んだため、サイズが肥大化しビルド時間が長くなった。
   - 要望に応えるために採用したとはいえ、Ansibleでのコンテナのプロビジョニングはイメージレイヤの増加を考えると得策とは言い難かった。
- テスト結果などはコンテナ内に含めるわけにはいかず(含めるとコンテナの再起動で結果が失われるため)、bind mountによりDocker外のディスクに出すようにしていたが、これのバックアップを取る仕組みがなかった。
   - 前述のディスク障害の際には、幸いにも壊れたハードディスクからサルベージすることができた。
- コンテナは全て1台のマシンで起動するため、slaveの数をある程度増やすためにはかなり性能の良いマシンが求められた。
   - クラスタをサポートするオーケストレーションツール(ex. Docker swarm, nomad, Kubernetes)が出てくる前(or 成熟する前)だったため、致し方ない面がある。

## CI環境のコード化リニューアル(2016/05〜2019/05)

[前述のCI環境のコード](#ci環境のコード化201504)の整理、及びクラスタ化の導入を目的とした。
個人プロジェクトとしてスタートしたが、実運用に至らず開発を停止した。

### 技術スタックと選定理由

- Ansible: コンテナイメージのプロビジョニングを行う
   - 1つのPlaybookにベタ書きしていた処理をAnsible roleに分割することで、可読性の向上を狙った。
   - 前回採用していたPackerは採用せず、事前に起動したコンテナに対してAnsibleのDocker connection plugin経由でプロビジョニングを実行するように変更した。
      - Packerの問題点: プロビジョニングの途中で失敗した場合、失敗した地点から実行を再開する手段がなかった。そのため、プロビジョニングコード作成時のトライアンドエラーがスムーズに行えなかった。
      - 今回の方式: コンテナは起動し続けているため、処理をべき等に書いている限りは実行に成功した箇所はスキップされる。そのため、失敗した地点まで短時間で戻ってこれる。またAnsibleのタグを設定することで指定したプロビジョニングだけ実行することもできる。
- Serverspec: プロビジョニングコードのテストを書く
   - Ansible role毎にテストを作成、Travisにより実行する。
   - プロビジョニングのユニットテストという位置付けを意図していた。
- Consul/Nomad: コンテナ環境のクラスタ化
   - クラスタの規模が小さく、Kubernetesは牛刀と考え選択しなかった。
   - 判断した時期は、まだAWS/GCPがKubernetesマネージドサービスの提供を行っていなかった。今ならばこれらの利用を前提として、Kubernetesを選択するだろう。

### もたらした改善/成果

実運用に至っていないため、個人的な成果となる。

- 作業で得た知見をオープンソースとして公開した。
   - Ansible role
      - https://github.com/FGtatsuro?tab=repositories&q=ansible&type=&language=
      - https://galaxy.ansible.com/FGtatsuro
   - Consul/Nomadクラスタのテンプレート
      - https://github.com/FGtatsuro/cluster

### 残った/生じた課題

- コンテナの思想からズレているという問題はそのまま残った。
   - 途中からこの事実を強く意識するようになった結果、開発が停滞し最終的には停止に至った。

## CI環境のクラウド上への移行(2020/04〜)

以下を契機として、オフィス内のサーバで動かしていたCI環境をクラウド上に移行し、これを主導した。

- 新型コロナウィルスの流行に伴いリモートワーク中心となったため、停電後の復旧対応が困難になった。
- オフィス移転に伴い旧オフィスにあったサーバルームが削減された。サーバルームにあったCI環境のサーバは執務エリアと同居するラックに格納されていたが、2Uサーバであったため稼動音が無視できないレベルであった。
   - 管理部の社員は出社することが多く、ラックが(よりによって)それら社員の席の近くに設置されていた。

### 技術スタックと選定理由

- GCP: Compute Engineインスタンス、ネットワークなどCI環境の構築に必要なリソースの作成
   - AWSとの選択となったが、以下の理由によりGCPを採用した。
      - AWS/GCPともにチームでは今まで使っておらず、連携を考える既存のリソースはなかった。AWSに既存のリソースがあればそちらを選択した。
      - AWS/GCPともに詳しいメンバーがいなかった。AWSに通じているメンバーがいればそちらを選択した。
      - 後述の [Kubernetes上でのテスト実行](#kubernetes上でのテスト実行202011検証中) を視野に入れており、両者のマネージドサービス(AWS: EKS/GCP: GKE)を比較した結果、GKEの方が扱いやすいと判断した。
         - EKSに比べ、GKEはKubernetesが隠蔽されて『いない』と考えた。一見これはデメリットのように見えるが、マネージドサービスとはいえKubernetesについて理解することは必要であり、チームでの学習を考えると隠蔽されている部分が少ない方が良いと判断した。
- Terraform: GCPリソースのコード化
   - GCPにはDeployment Managerという類似のサービスがありそちらも動かして検証した結果、以下の理由でTerraformを採用することとした。
      - planによる差分表示、リソースの依存関係の処理、既存リソースのインポートなど、Terraformが機能的に大きく勝っていた
      - Deployment Managerではサポートされておらず、Terraformでサポートされているリソースが存在した(逆は観測範囲では確認できず)

### もたらした改善/成果

- GCP/Terraformに関するチーム力の向上
   - クラウドの導入はチームとしてインパクトが大きいと考え、新しい技術の啓蒙をこれまで以上に意識して実施した。
      - ペアレビューやモビングセッションを積極的に実施する
      - Terraformコードレビューのルールを策定する(ex. terraform planの結果/terraform apply後のspec test実行の結果 をissueに貼る)
   - 結果として、以下をはじめとするいくつかのタスクについて、チームメンバーにこなしてもらう事ができた。
      - Cloud ArmorによるCI環境へのアクセス制御
         - クラウド移行前は、社内ネットワークに置いていたため必要なかった。
      - CI環境のディスクのスナップショット設定
         - [CI環境のコード化](#ci環境のコード化201504)で上げたバックアップ問題の解消

### 残った/生じた課題

- [CI環境のコード化](#ci環境のコード化201504)で上げた『slaveの数をある程度増やすためにはかなり性能の良いマシンが求められる』という問題がよりシビアに顕在化した。
   - slaveの数を以前と同等にするのはコスト的に厳しく、結果としてテストの同時実行数を減らさなくてはならなかった。
   - スケールアウトに比べてスケールアップの方がコスパが悪い。

## Kubernetes上でのテスト実行(2020/11〜(検証中))

現状のCI環境が抱える以下の問題を解決することを目的としている。

- 『slaveの数をある程度増やすためにはかなり性能の良いマシンが求められる』

個人プロジェクトとしてスタートした後、チームのプロジェクトに組み込まれて現在検証中である。

### 技術スタックと選定理由

- Kubernetes: Jobリソースを使い、テストを実行する。
   - Kubernetes以外で検討したテスト実行環境は以下の通り。
      - Cloud Build: 東京リージョンを選択できない。ネットワーク通信がメインとなるテストを流しているため、レイテンシの増加がテスト実行時間に与える影響が大きい。
      - Cloud Run: 15分の時間制限がある。インテグレーションテストをメインに流しているため、15分の制約が守れないジョブの方が多い。
      - Github Actions: iOSのテストを実行するために使用している。macOSノードのコストはLinuxノードに比べてかなり大きいため、実行時間は可能な限りiOSのテスト実行に回したい。
- GKE: Kubernetesのマネージドサービス
   - 選定理由は[CI環境のクラウド上への移行](#ci環境のクラウド上への移行202004)参照

### もたらした改善/成果

- コンテナイメージの改善
   - Kubernetes上で動かすにあたり、[CI環境のコード化](#ci環境のコード化201504)の技術的負債である肥大化したコンテナイメージの解消を進めている。
      - プリエンプティブルノードを使ってコストを抑えている関係上、コンテナイメージのキャッシュを当てにすることができない(24hでノードが死ぬため)。よって、イメージサイズを小さくすることが求められる。
      - テスト毎に必要なランタイムのみ含んだイメージを作成する
      - Ansibleによるプロビジョニングを廃止する
      - マルチステージビルドによる依存関係解決と実行の分離
- テストの実行をslaveからKubernetes上に移行する
   - PoC完了
   - 現状のCI環境からslaveコンテナをなくすことで、マシンの性能を大幅に下げることができコスト削減に繋がる。
   - KubernetesのAutoScaleにより、テスト実行時に自動でノードが追加される。
- [Autopilot mode](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview)の導入
   - プリエンプティブルでないノードで構成されるため、StatefulSetの導入が容易に行えるようになった。
      - https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices
   - プリエンプティブル/非プリエンプティブルノードを組み合わせたクラスタと比べて、コストを削減することに成功した。

### 残った/生じた課題

検証中
