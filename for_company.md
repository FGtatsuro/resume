# 社内システム/ツールの開発

開発を補助するシステム/ツールの作成を行った。

---

## 課題管理簿(社外)とissueトラッカー(社内)の連携(2011/02〜)

社外からメールで送信されてくる課題管理簿と社内のissueトラッカーを連携するシステムを開発した。
提供する機能は以下の通り。

- 課題管理簿の受信を検知する。
- 課題管理簿の内容を解析し、issueトラッカーに登録する。
   - 既にissueが作成してある課題管理簿に対して返信があった場合、該当するissueに対してコメントで追記する。
- 課題管理簿をやり取りするメールの内容により、該当するissueをクローズする。

### 背景

このシステムを作る前は以下のフローで課題管理を行っており、やや面倒であった。

1. 社外から課題管理簿を添付したメールが担当者に届く。
2. 担当者は課題管理簿(パスワード付きzipで圧縮されている)を解凍し、内容を確認する。
3. 内容を確認した後に、課題管理簿を更新し、社外へのメールに添付する。
4. (必要であれば)社内のissueトラッカーに課題管理簿の内容を転記する。

このうち、手順1, 2, 4は社内で閉じた手順であり自動化が可能だろうと考え、個人プロジェクトとしてPoCを始めた。
その後、社内で実稼動することとなった。

### 技術スタックと選定理由

- Python
   - 主な処理
      - メールの受信検知
      - 課題管理簿の解析(Excelの解析)
      - クローズ判定
      - 登録するissueの内容作成(Textile記法)
      - issue登録・更新APIの実行
   - 実行環境(Linux)、開発環境(OSX)ともにデフォルトでインストールされている、高い性能は必要ないためインタプリタ言語で問題ないだろう、という理由で採用
- Redmine
   - 当時社内で使用していたissueトラッカー。変更する強い理由もなかったため、そのまま採用

### もたらした改善/成果

- 前述の手順1, 2, 4が自動化され、担当者の手間が大きく削減された。
- 担当者の裁量で転記していた内容が全てissueトラッカー上に集約されることになり、課題内容を把握・整理する必要があったQAチームのリーダに喜ばれた。
- 自身のスキル向上
   - Shift-JIS(課題管理簿), euc-jp(動作環境のサーバ), ISO-2022-JP(メール本文), UTF-8(その他)、といった複数の文字符号化方式を扱う経験を得た。
   - 当時のPython(2.x)はマルチバイト文字列の処理が今と比べてやや煩雑であり、その点は苦労した。
      - 後に文字列処理に関してブログにまとめた。
         - Python2: https://qiita.com/FGtatsuro/items/cf178bc44ce7b068d233
         - Python3: https://qiita.com/FGtatsuro/items/f45c349e06d6df95839b

### 残った/生じた課題

- 手順3は自動化できないまま残った。
   - 当時は自身の技術力の関係で自動化できなかったが、今ならばこれも自動化できるかもしれない。
      - ex. issueに対して返信すると、その内容を含んだ課題管理簿を生成、メールで返信する
      - ただし、後に別件でPythonのライブラリをいくつか試した結果、Excel処理は読み込みはともかく書き込みにクセがあることが分かった。そのため、実装には苦労するかもしれない。
         - ライブラリによって微妙に挙動が違う
         - 書き込むデータによっては、生成したシートがExcelで開けない場合がある
- Close判定を正規表現によるテキストマッチングで実装していたため、偽陽性率/疑陰性率がそれなりに高かった。
   - 今実装するとしたら、close判定に関する学習を追加し、偽陽性率/疑陰性率を下げられないかを検証するだろう。
- 処理の責任分割が上手くされておらず、コードの可読性/メンテナンス性は高くなかった。
   - 今実装するとしたら、少なくとも解析処理と登録処理は別プロセスにするだろう。
      - メッセージキューを使い疎結合にする。

## 機能レベルでのチェックツール(2013/11〜)

システムが提供している機能が正しく機能しているかをチェックするためのツールを作成した。

### 背景

オペレーションチームから、以下のような問題を解決できないか相談を受けた。

- コンポーネントAとコンポーネントBを含むシステムが提供する機能が正しく動いていない(ex. REST APIがstatus 4xx/5xx)
   - コンポーネントAとコンポーネントBはヘルスチェックレベルで問題なく動いている。
      - L3レベルのヘルスチェック
         - ex. pingに応答する
      - L4レベルのヘルスチェック
         - ex. サービスのポートが開いている
      - L7レベルのヘルスチェック
         - ex. ヘルスチェック用のREST APIがstatus 200を返す
   - 原因例
      - コンポーネントAの設定が間違っており、コンポーネントBを正しく認識していない。
      - コンポーネントBの権限設定が正しく行われておらず、依存するAWSリソースへのアクセスが拒否される。

上記問題の解決のため、『ヘルスチェックよりもアプリケーションに近いレイヤでのチェックをする』ツールが必要であると判断した。

加えて、以下の要件にも対応する必要があった。

- 各環境毎に提供する機能が異なる可能性があるため、実行するチェックを柔軟に組み変えたい。
   - ex. 環境Aでは機能1が提供されているが、環境Bではその機能がサポートされていない。

### 技術スタックと選定理由

- Java: v1
   - QAが既に作成しているインテグレーションテスト(Java)を流用して作成した。
      - 『機能を保証するために必要な最低限のテスト』を選定し、別プロジェクトに切り出した。
   - 目的は達成できたが、実際に運用した結果、以下に示す問題があることが分かった。
      - 実行にかかる時間が長い
         - ビルド時間が支配的だった
      - 環境毎にブランチを作成して、不必要なテスト(=サポートしていない機能に関するテスト)を適宜削除する必要があった。
         - 共通部の変更は全てのブランチをアップデートする必要があり手間がかかる。
- Python: v2
   - v1で発生した問題を解決するものとして作成した。
      - インタプリタ言語にしたため、ビルドが不要になり実行にかかる時間は削減された。
         - チェック内容はREST APIへのアクセスが中心であったため、Java->Pythonに移行したことによるチェック自体の時間増加は大きな問題とはならなかった。
      - テストフレームワークとして採用した pytest の機能を活用することで、単一ブランチで複数環境へのテスト実行を可能とした。
         - 各テスト毎に『どのコンポーネントが必要か』『どの機能に関するテストか』などの情報を示すマーカーを付与する。
         - 環境毎の設定ファイルを作成し、その環境がサポートするコンポーネント/機能に関する情報を前述のマーカーで表現し、設定ファイルに記述する。
         - 各テストの実行前に処理されるフックポイントで、テスト実行判定のロジックが実行される。
            - テストに設定されているマーカーが、環境の設定ファイルに全て含まれる場合のみテストを実行する。
         - テストを実行する環境は、コマンドラインにより指定する。

### もたらした改善/成果

- オペレーションチームのデプロイ作業の効率化に寄与した。
   - 以前はデプロイが成功したかを確認するには、インテグレーションテストを流すしかなかった。
      - インテグレーションテストは全てのコンポーネントが存在する前提で書かれているため、デプロイ作業の途中で確認する術がない
      - インテグレーションテストの実行には時間がかかる。
   - このツール以後は、デプロイ作業を分割し都度確認をしながら進めることができるようになった。
      - ex. アップデートするコンポーネントのインスタンスをロードバランサから外しチェックツールで確認、問題なければロードバランサに再接続 => 以降、全てのインスタンスについて同様の作業を繰り返す
- 自身のスキル向上
   - 別のチームに使ってもらう事が前提だったため、ドキュメントは普段以上に整備した。結果として、使用や設定追加だけでなく、その後の機能追加も他者にやってもらえるようになった。
   - ここでpytestに慣れ親しんだことが、この後に増えたサーバのテストにPythonを採用する契機となった。

### 残った/生じた課題

- 自分が想像していたよりも、バージョン差異(2系/3系だけでなくマイナーバージョン(特に2系)の違いでも)やライブラリ依存関係回りで、トラブルを抱えることが多かった。
   - Pythonはベンダリングを(少なくとも言語レベルでは)サポートしていないため、新しい実行環境では上手く動かない、というケースが度々あった。
   - 今実装するとしたら、Docker化を検討するだろう。
   - Golangによるシングルバイナリ化も検討対象に入るか。その場合、pytest相当のテストランナーをどう実現するかが重要になる。

## image-cleaner(2020/09〜)

Registry内のイメージのうち、タグがついていないものを自動で削除するサービス(image-cleaner)を作成した。

### 背景

特定のタグをつけずに(=常にlatestを使用する)コンテナイメージをRegistryに格納する場合、更新の度にタグがついていないイメージが1つ増えることになる。
コスト的には大した問題はないが、無駄であることには違いないため、気付いたタイミングで手動削除を実施していた。

### 技術スタックと選定理由

- Container Registry(GCP)
   - コンテナイメージを格納する
- Cloud Run
   - image-cleanerを実行し、外部に認証付きのRESTエンドポイントを公開する
- Cloud Scheduler
   - Cloud Runにより公開したRESTエンドポイントを一定周期(1回/day)で実行する
- Cloud Build
   - image-cleanerのコンテナイメージをビルドし、Container RegistryにPushする。
   - リポジトリのmainブランチに変更がマージされるのを契機に、ビルドが実行される。
- Golang
   - image-cleanerの実装言語
   - 以下の理由により採用
      - [Container Registryにアクセスするライブラリ](https://github.com/google/go-containerregistry)がGolangで実装されていた
      - [Kubernetesの検証を始める予定](./infra.md#kubernetes上でのテスト実行202011検証中) があり、その前にGolangに関する知識を入れておきたかった。
         - Kubernetes本体や多くの周辺ツールがGolangで実装されており、調査のためにはGolangを読む力が必要になると判断した。

### もたらした改善/成果

- 自身のスキル向上
   - Cloud Runを用いたサービス公開のパターンを確立した。これにより、[後続のサービス](#ci-vacationci-starter202009)の開発がスムーズに進んだ。
   - 採用理由の通り、Golangに関する知識を入れることができ、[Kubernetesの検証](./infra.md#kubernetes上でのテスト実行202011検証中)に生かすことができた。
   - 作成過程で発生した問題を元に、オープンソースへのPRを作成しマージされた。
      - https://github.com/google/go-containerregistry/pull/880

### 残った/生じた課題

- [デプロイパイプラインの構築](#デプロイパイプラインによる社内ドキュメントの自動公開202102) を経験して、そもそもタグがついていないイメージを作らない方が良いという結論に至った。そのため、このサービスは意味の薄いものとなった。
   - GithubのSHAと関連つけたタグをつければ、関連するコードの特定が容易になり障害時の調査がしやすくなる

## ci-vacation/ci-starter(2020/09〜)

以下の2つのサービスを作成し、[GCP上のCI環境](./infra.md#ci環境のクラウド上への移行202004)をGoogleカレンダーの予定に合わせて起動/停止するようにした。

- ci-vacation: このサービスを実行した日が以下のカレンダーに含まれているかを判定する
   - CI環境を停止する日を入れたカレンダー。現在は土日を登録している。
   - 祝日のカレンダー
- ci-starter: ci-vacationの結果を受けて、Compute Engineインスタンスを停止/起動する

### 背景

GCP上のCI環境は、コストの関係で同時実行数を減らすなどの対応をしていた。
しかし他の方法でコストを削減することができれば、同時実行数を戻すという選択肢も取れるのではないかと考えていた。

### 技術スタックと選定理由

- Cloud Run
   - ci-vacation/ci-starterを実行し、外部に認証付きのRESTエンドポイントを公開する
- Cloud Scheduler
   - Cloud Runにより公開したci-starterのRESTエンドポイントを一定周期(1回/day)で実行する
- Cloud Build
   - ci-vacation/ci-starterのコンテナイメージをビルドし、Container RegistryにPushする。
   - リポジトリのmainブランチに変更がマージされるのを契機に、ビルドが実行される。
- Golang
   - ci-vacation/ci-starterの実装言語

### もたらした改善/成果

- CI環境のコストを数千円(/月)削減することに成功した。
   - 検討の結果、現状の同時実行数でもさほど問題ないと判断したため、コストダウンの恩恵に預かることができた。
- 自身のスキル向上
   - Cloud Runを用いたサービス公開のパターンをブラッシュアップすることができた。
      - Cloud Runから別のCloud Runを呼ぶ場合の認可設定
      - Cloud Build上でプライベートリポジトリに置いたGolang moduleを使う設定

### 残った/生じた課題

- ci-vacation: 現状で決め打ちしている以下の項目を、実行時に変更できると応用の幅が広がると考える。
   - 対象とするカレンダー
   - カレンダーに含まれるかの判定対象となる日付

## デプロイパイプラインによる社内ドキュメントの自動公開(2021/02〜)

https://cloud.google.com/kubernetes-engine/docs/tutorials/gitops-cloud-build を参考にデプロイパイプラインを構築し、社内ドキュメントに変更があれば自動的に最新版を公開するようにした。

### 背景

- [CI環境のGCP上への移行](./infra.md#ci環境のクラウド上への移行202004)と同時期に、社内のマシンで公開していたドキュメント(ISO認証のために整備する必要があった)も移行を検討していた。
- 検証も兼ねて、[以前に作成したKubernetesクラスタ](./infra.md#kubernetes上でのテスト実行202011検証中)上に移行した。
- 移行後、しばらくは手動でマニフェストを更新しデプロイすることで最新版を公開していたが、この作業は自動化が可能だと考えた。

### 技術スタックと選定理由

- Terraform
   - ドキュメント公開に必要なGCPリソースを作成する
      - static IPアドレス
      - DNSゾーン/レコード
      - Cloud Armor: IPアドレスによるアクセス制限
      - Identity-Aware Proxy: Google Workspaceのドメインによるアクセス制御
   - 類似のツールとして、KubernetesリソースとしてGCPリソースを管理する[Config Connector](https://cloud.google.com/config-connector/docs/overview)の導入も検討したが、以下の理由によりTerraformを採用した。
      - Terraformの導入も一年以内のことであり、ようやくチームでの利用に慣れた頃合いであった。そのタイミングでツールを変更することは学習コストの面で負荷が高く反発を招くと考えた。
      - Terraformと比較して、Config Connectorはまだドキュメントや参考となる事例が充実しておらず、トラブルが発生したときの調査コストが高いと判断した。
- Kubernetes
   - ドキュメント公開に必要なKubernetesリソースを作成する
      - Ingress
      - Service
      - Deployment
      - SSL証明書
- Cloud Build
   - ドキュメント用リポジトリとKubernetesマニフェスト用リポジトリで別個のビルド設定を持つ。
   - ドキュメント用リポジトリ
      - Sphinxで構成された社内ドキュメントのコンテナイメージをビルドし、Container RegistryにPushする。
      - 上記イメージにGithubのコミットに紐付いたタグを付与する。
      - Kubernetesのマニフェストを上記タグを参照するよう変更し、Kubernetesマニフェスト用リポジトリにコミットする。
   - Kubernetesマニフェスト用リポジトリ
      - masterブランチの更新をトリガとして、変更されたKubernetesマニフェストをapplyする。

### もたらした改善/成果

- 自分自身に関係するToil(手動でのマニフェスト更新->デプロイ)を削減することができた
- 自身のスキル向上
   - Terraform + Kubernetes + GCPによる『(必要であればアクセス制限をかけた)HTTPSでのサービス公開』のパターンを確立した。

### 残った/生じた課題

- 現状は Terraform/Kubernetesの設定に固有情報がベタ書きされているが、それぞれの汎化する仕組みを用いて切り出せないかを検討している。
   - Terraform: [Modules](https://www.terraform.io/docs/language/modules/index.html)
   - Kubernetes: [Kustomize](https://kubectl.docs.kubernetes.io/references/kustomize/)
